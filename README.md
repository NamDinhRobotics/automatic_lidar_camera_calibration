# automatic_lidar_camera_calibration
## Overview
This is a package for automatic extrinsic calibration between a 3D LiDAR and a camera, described in paper: **3D LiDAR Intrinsic Calibration and Automatic System for LiDAR to Camera Calibration** ([PDF](https://github.com/UMich-BipedLab/automatic_lidar_camera_calibration/blob/release_v1/AutomaticCalibration.pdf)). This system for target-based automatic LiDAR to camera extrinsic calibration is given. Specifically, using the back end from this [paper](https://arxiv.org/pdf/1910.03126v1.pdf), a front end is developed that automatically takes in LiDAR and camera data for diamond-shaped planar targets, synchronizes them, extracts the LiDAR payloads and AprilTag corners for the camera images, and then passes the data to the back end that produces
the rigid body transformation between the camera and LiDAR.

* Authors: Bruce JK Huang, Chenxi Feng, Madhav Achar, Maani Ghaffari, and Jessy W. Grizzle
* Maintainer: [Bruce JK Huang](https://www.brucerobot.com/), brucejkh[at]gmail.com
* Affiliation: [The Biped Lab](https://www.biped.solutions/), the University of Michigan

This package has been tested under **MATLAB 2019a** and **Ubuntu 16.04**.

**[Issues]**
If you encounter _any_ issues, I would be happy to help. If you cannot find a related one in the existing issues, please open a new one. I will try my best to help! 


## Abstract
Periodic intrinsic and extrinsic (re-)calibrations are essential for modern perception and navigation systems deployed on autonomous robots. To date, intrinsic calibration models for LiDARs have been based on hypothesized physical mechanisms for how a spinning LiDAR functions, resulting in anywhere from three to ten parameters to be estimated from data. Instead we propose to abstract away from the physics of a LiDAR type (spinning vs solid state, for example) and focus on the spatial geometry of the point cloud generated by the sensor. This leads to a unifying view of calibration. In experimental data, we show that it outperforms physics-based models for a spinning LiDAR. In simulation, we show how this perspective can be applied to a solid state LiDAR. We complete the paper by reporting on an open- source automatic system for target-based extrinsic calibration from a LiDAR to a camera.

## Introduction Video
Please checkout the introduction [video](https://www.brucerobot.com/automatic-calibration). It highlights some importants keypoints in the paper!  

[<img src="https://github.com/UMich-BipedLab/automatic_lidar_camera_calibration/blob/release_v1/figure/IntroVideo.png" width="960">](https://www.brucerobot.com/automatic-calibration)


## System Diagram Overview
A system diagram for automatic intrinsic and extrinsic calibration. The top shows the front-end of the pipeline. Its input is raw camera and LiDAR data, which are subsequently synchronized. The AprilTag and LiDARTag packages are used to extract the target information, which is then examined to remove outliers and ensure that targets are still properly synchronized. Each run of the front-end saves all related information into a ROS bagﬁle, which can then be sent to the back-end for further processing. The back-end takes in (possibly many) ROS bagﬁles and does the following: (i) reﬁnes the image corners; and (ii) extracts vertices of the LiDAR targets. The correspondences are established between corners and vertices and an extrinsic transformation is determined PnP as in this [paper](https://arxiv.org/pdf/1910.03126v1.pdf). For intrinsic calibration, the resulting vertices of the LiDAR targets are used to extract normal vectors and a point on the plane. The calibration parameters are determined to minimize the P2P distance from the plane to the target points provided by the LiDARTag package.

<img src="https://github.com/UMich-BipedLab/automatic_lidar_camera_calibration/blob/release_v1/figure/automaticCalibrationPipeline.png" width="960">


## Application Videos 
The 3D-LiDAR map shown in the videos used this package to calibrate the LiDAR to camera (to get the transformatoin between the LiDAR and camera). Briefly speaking, we project point coulds from the LiDAR back to the semantic labeled images using the obtained transformation and then associate labels with the point to build the 3D LiDAR semantic map.

[Halloween Edition: Cassie Autonomy](https://www.youtube.com/watch?v=4OUr2DspYoo) 

[Autonomous Navigation and 3D Semantic Mapping on Bipedal Robot Cassie Blue (Shorter Version)](https://www.youtube.com/watch?v=uFyT8zCg1Kk)

[Autonomous Navigation and 3D Semantic Mapping on Bipedal Robot Cassie Blue (Longer Version)](https://youtu.be/N8THn5YGxPw)
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/Halloween.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/3D-LiDAR-Semantic-maps.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/3D-LiDAR-Semantic-maps2.png" width="640">



## Results Quick View
Using the obtained transformation, LiDAR points are mapped onto a semantically segmented image. Each point is associated with the label of a pixel. The road is marked as white; static objects such buildings as orange; the grass as yellow-green, and dark green indicates trees.

<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/semanticImg.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/semanticPC3.png" width="640">

# Why important? 
A calibration result is not usable if it has few degrees of rotation error and a few percent of translation error.
The below shows that a calibration result with little disturbance from the well-aigned image.

<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/disturbance.png" width="640"> 
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/undisturbance.png" width="640">


## Calibration Targets
Any **square** targets would be fine. The dimensions are assumed known.
note: You can place any number of targets with different size in different datasets.

## Requirements
1. The front-end
* Please download the follow packages and place them under a catkin workspace:
    - [_sync_lidartag_apriltag_](https://github.com/UMich-BipedLab/sync_lidartag_apriltag)
    - [_alignment_msgs_](https://github.com/UMich-BipedLab/alignment_msgs)  
    - [_LiDARTag_](https://github.com/UMich-BipedLab/LiDARTag)
    - [_LiDARTag_msgs_](https://github.com/UMich-BipedLab/LiDARTag_msgs)  
    - [_AprilTag_ROS_](https://github.com/UMich-BipedLab/AprilTag_ROS)
    - [_AprilTag_msgs_](https://github.com/UMich-BipedLab/AprilTag_msgs)
    
2. The back-end
* Which toolboxes are used in this package:
    - MATLAB 2019a
    - optimization_toolbox
    - phased_array_system_toolbox
    - robotics_system_toolbox
    - signal_blocks

## Dataset
For the front-end, please download from [here](https://drive.google.com/drive/folders/1MwA2dn6_U3rCWh9gxCe8OtWWgSxriVcW?usp=sharing).  
For the back-end and the optimization process, please download from [here](https://drive.google.com/drive/folders/16wu22qMxVC58mx7xyczAk3z93kFGOdfd?usp=sharing).


## Running
All the datasets have to be downloaded.
1. The front-end  
In the _sync_lidartag_apriltag_ package, the sync_cam_lidar launch file should be ran first, and then run the alignment_node_only launch file to run the tag pairing node. The _alignment_msgs_ will be published as output, and one can record them.

    
2. The back-end  
Onces all the data have been processed by the front-end node, i.e., saved as (possibly many) ROS bagfiles, please place them under a folder and change the [path.bag_file_path](https://github.com/UMich-BipedLab/automatic_lidar_camera_calibration/blob/e74382f4283576702a271fdbfe526b48c0c1acdb/automatic_calibration_main.m#L105) in [_automatic_calibration_main.m](https://github.com/UMich-BipedLab/automatic_lidar_camera_calibration/blob/release_v1/automatic_calibration_main.m)_ and run. 

<!--
**[Super Super Quick Start]**
Just to see the results, please clone this repo, download the [process/optimized data](https://drive.google.com/drive/folders/1DTyG9pcIvXBqgXUxULWUaBT1zxLYmfz7?usp=sharing) into ALL_LiDAR_vertices and change the [path.load_dir](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/901a5b4ff4a054b3f19ebb386ef1bfcd4f8c334d/main.m#L49) to ALL_LiDAR_vertices in main.m, and then hit run!

**[Super Quick Start]**
If you would like to see how the LiDAR vertices are optimized, please place the [test datasets](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/tree/master#dataset) in folders, change the two paths ([path.bag_file_path](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/901a5b4ff4a054b3f19ebb386ef1bfcd4f8c334d/main.m#L50) and [path.mat_file_path](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/901a5b4ff4a054b3f19ebb386ef1bfcd4f8c334d/main.m#L51)) in main.m, and then hit run!


**[Calibrators]**
- Please first try the [Super Super Quick Start] section to make sure you can run this code.
- Use _justCalibrate.m_ file
- Find out your camera intrinsic matrix and write them in the _justCalibrate.m_ file. 
- Give initial guess to the LiDAR to camera transformation
- Edit the _trained_ids_ and _skip_indices_ (ids are from _getBagData.m_).
- If you have more validation dataset (containing targets), set the _validation_flag_ to 1 and then use put the related information to _getBagData.m_.
- Place several _square_ boards with known dimensions. When placing boards, make sure the left corner is taller than the right corner.
- Use you favorite methods to extract corners of camera targets and then write them in _getBagData.m_. When writing the corners, Please follow **top-left-right-bottom** order. 
- Given point patches of LiDAR targets, saved them into .mat files and also put them _getBagData.m_. Please make sure you have correctly match your _lidar_target_ with _camera_target_. 
- RUN _justCalibrate.m_! That's it!

note: You can place any number of targets with different size in different datasets.

**[Developers]**
Please download all datasets if you would like to play around.
-->

# Qualitative results
For the method GL_1-R trained on S_1, the LiDAR point cloud has been projected into the image plane for the other data sets and marked in green. The red circles highlight various poles, door edges, desk legs, monitors, and sidewalk curbs where the quality of the alignment can be best judged. The reader may find other areas of interest. Enlarge in your browser for best viewing. 


<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test1_3.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test2_3.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test3_3.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test4_3.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test5_3.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test6_3.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/test7_3.png" width="640">

# Quantitative results
For the method GL_1-R, five sets of estimated LiDAR vertices for each target have been projected into the image plane and marked in green, while the target's point cloud has been marked in red. Blowing up the image allows the numbers reported in the table to be visualized. The vertices are key.

<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v1-2.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v2-2.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v3-2.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v4-2.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v5-2.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v6-2.png" width="640">
<img src="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/figure/v7-2.png" width="640">

## Citations
1. Jiunn-Kai Huang, Chenxi Feng, Madhav Achar, Maani Ghaffari, and Jessy W. Grizzle, "Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs" ([arXiv](https://arxiv.org/abs/2012.03321))

```
@article{huang2020global,
  title={Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs},
  author={Huang, Jiunn-Kai and Feng, Chenxi and Achar, Madhav and Ghaffari, Maani and Grizzle, Jessy W},
  journal={arXiv preprint arXiv:2012.03321},
  year={2020}
}
```
2. Jiunn-Kai Huang and J. Grizzle, "Improvements to Target-Based 3D LiDAR to Camera Calibration" ([PDF](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration/blob/master/LiDAR2CameraCalibration.pdf))([arXiv](https://arxiv.org/abs/1910.03126))
```
@article{huang2019improvements,
  title={Improvements to Target-Based 3D LiDAR to Camera Calibration},
  author={Huang, Jiunn-Kai and Grizzle, Jessy W},
  journal={arXiv preprint arXiv:1910.03126},
  year={2019}
}
```
3. Jiunn-Kai Huang, Maani Ghaffari, Ross Hartley, Lu Gan, Ryan M. Eustice,
and Jessy W. Grizzle, "LiDARTag: A Real-Time Fiducial Tag using
Point Clouds" ([PDF](https://github.com/UMich-BipedLab/LiDARTag/blob/release_v0/LiDARTag.pdf))([arXiv](https://arxiv.org/abs/1908.10349))
```
@article{huang2020improvements,
  author={J. {Huang} and J. W. {Grizzle}},
  journal={IEEE Access}, 
  title={Improvements to Target-Based 3D LiDAR to Camera Calibration}, 
  year={2020},
  volume={8},
  number={},
  pages={134101-134110},}
```

